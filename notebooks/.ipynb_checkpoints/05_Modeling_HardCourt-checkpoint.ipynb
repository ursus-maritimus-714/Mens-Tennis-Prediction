{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa24a63",
   "metadata": {},
   "source": [
    "## Predictive Modeling (Hard Court Version)\n",
    "\n",
    "### Summary of Previous Stage (see Notebook 4; Preprocessing_Training_Hardcourt)\n",
    "\n",
    "#### 5 linear benchmark models (75%/25% Train/Test split) were generated:\n",
    "* 1) A \"dummy\" model, which simply uses the mean of the training data split to predict the target of % points won by a given player in a given match\n",
    "    * Root Mean Squared Error (RMSE) Training Set: 6.21%  Test Set: 6.11% \n",
    "* 2) A multivariate linear model that uses as predictive features only 1- difference in ranking between the two players, 2- difference in log of the rankings of the two players, and 3- difference in rankings points between the two players. Given that tournament entry and seeding are determined primarily by ranking data by the tour itself, this simple model can very prudently be seen as a fair benchmark for any system hoping to predict player performance on a move-forward basis.\n",
    "* 3) A univariate linear model using implied win probabilities (IWPs) derived from aggregate closing wagering lines from a number of reputable sportsbooks (exact books aggregate vary by specific match) as predictive features. \n",
    "* 4) A univariate linear model using IWPs derived only from Pinnacle Sports' closing wagering lines as predictive features. Pinnacle is considered to be one of the \"sharpest\" sportsbook, with closing lines typically being highly efficient.\n",
    "* 5) A univariate linear model using only Opening Line data from Pinnacle Sports (via Oddsportal). Though not as efficent as the closing lines, which reflect the sum total of the \"wisdom of the markets\" (including insider info) right up to match time, these lines are still quite efficient. \n",
    " \n",
    "### High-Level Overview of Hard Court Modeling\n",
    "\n",
    "Prediction is to be carried out at the player level for a given match (so each match in the sample has two records associated with it). Performance features used for prediction of a given player's % pts won in a given match are accrued based on  surface-specific (with a few exceptions) match data in matches PRIOR TO BUT NOT INCLDING the match being predicted nor any subsequent matches. \n",
    "\n",
    "The large majority of features included in the modeling stage are \"differential\", meaning that their values result from subtraction of previously calculated values for each of the two players in the match being predicted (Player X - Player Y or Player Y - Player X). As a simple example (actual features are typically adjusted in a number of ways before this stage) if Player X won 55% of his previous service points and his opponent (Player Y) won 47% of his own serve points the \"differential\" feature value \"player_past_service_pts_diff\" for Player X would be 55 - 47 = 8% and for Player Y would be 47 - 55 = -8%.  \n",
    "\n",
    "Based on the results of prevous modeling iterations, data from the years 2015-2019 are currently included in the hard court-specific model, with an additional prior 3 years (2012-2014) used in statistical accrual for feature generation. Also, a threshold of minimum of 20 prior matches for BOTH players in a given match to be predicted is employed. Not surprisingly, prediction accuracy is sensitive to the amount of data available to generate predictive features, as well as to the amount of data available to train and test the model. Critically, matches filtered out at the current point and beyond (ie, during modeling) WERE used during statistical accruals for feature generation. Also of note, matches which lasted fewer than 12 games were filtered out BEFORE statistical accruals for feature generation.\n",
    "\n",
    "Four regression models are evaluated presently. First is simply a Linear Regression Model. The subsequent three models are decision tree models: Random Forest, Gradient Boosting Regressor and HistGradient Boosting Regressor. The latter is an ensemble machine learning algorithm that is very fast (relative to standard gradient boosting models), and performs well on heterogenous data sets. Boosting, generally, refers to a class of ensemble learning algorithms that add tree models to an ensemble sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b3b3a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133d431",
   "metadata": {},
   "source": [
    "### Load and Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d7aaa",
   "metadata": {},
   "source": [
    "Filtering parameters identical to in dummy/simple modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af618860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ths is the file and analysis data range used for the main hard court analysis\n",
    "df = pd.read_csv('../data/df_player_all_hard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15247e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data range used for the main analysis for hard courts- 2015-2019 in model; 2012-2014 used for additional stats accrual \"runway\"\n",
    "df_filter = df[~df['tour_wk'].str.contains(\"2012\")] \n",
    "df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2013\")]\n",
    "df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2014\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2015\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2016\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2017\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2018\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2019\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995cacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only matches played on hard courts\n",
    "#df_filter2 = df_filter.loc[(df_filter[\"t_surf\"] == 2)]\n",
    "#df_filter2 = df_filter.loc[(df_filter[\"t_surf\"] == 2) & (df_filter[\"p_matches_surf\"] > 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bdde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now also will remove BOTH players from individual matches remaining in the surface-specific sample already filtered by year range\n",
    "# where one or both players has played N or fewer matches prior to the one to be predicted on.\n",
    "# Threshold of minimum of 20 was established as optimal in the orignal Tennis Prediction Project\n",
    "df_low = df_filter.loc[df_filter['p_matches_surf'] <= 20, 'm_num']\n",
    "df_filter2 = df_filter[~df_filter['m_num'].isin(df_low)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f32222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell used only for final presentation figure where hard court sample N was reduced to be equal to that of the clay court sample\n",
    "# Best model (Gradient Boosting Regressor) was run at this N for that figure using the same hyperparameters as for full sample.\n",
    "#df_filter3 = df_filter3.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0170740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pared down to just the predictive features(both raw and player-opponent differential for match being predicted on), and the target feature itself ( player % pts won in the mtch being predicted on)\n",
    "# All features are derived from data available prior to any given match being predicted on. No data leakage!\n",
    "# This feature set DOES NOT INCLUDE implied odds (vig removed) from Dan Westin's historical wagering lines. See cell below for that iteration.\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_rank\", \"p_log_rank\", \"p_rank_pts\", \"p_ent\", \"p_hd\", \"p_ht\", \"p_age\", \"p_matches_surf\", \"p_H2H_w\", \"p_H2H_pts_won%\", \"p_pts_won%_l60_decay\", \"p_pts_won%_l60_decay_IO\", \"p_pts_won%_l10\", \"p_SOS_adj_pts_won%_l60_decay\", \"p_SOS_adj_pts_won%_l60_decay_IO\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted\", \"p_SOS_adj_pts_won%_l10\", \"p_sv_pts_won%_l60_decay\", \"p_sv_pts_won%_l10\", \"p_SOS_adj_sv_pts_won%_l60_decay\", \"p_SOS_adj_sv_pts_won%_l10\", \"p_ret_pts_won%_l60_decay\", \"p_ret_pts_won%_l10\", \"p_SOS_adj_ret_pts_won%_l60_decay\", \"p_SOS_adj_ret_pts_won%_l10\", \"p_ace%_l60_decay\", \"p_ace%_l10\", \"p_SOS_adj_ace%_l60_decay\", \"p_SOS_adj_ace%_l10\", \"p_aced%_l60_decay\", \"p_aced%_l10\", \"p_SOS_adj_aced%_l60_decay\", \"p_SOS_adj_aced%_l10\", \"p_bp_save%_l60\", \"p_bp_save%_l10\", \"p_SOS_adj_bp_save%_l60\", \"p_SOS_adj_bp_save%_l10\", \"p_bp_convert%_l60\", \"p_bp_convert%_l10\", \"p_SOS_adj_bp_convert%_l60\", \"p_SOS_adj_bp_convert%_l10\", \"p_pts_won%_std_l60_decay\",\"p_sv_pts_won%_std_l60_decay\", \"p_ret_pts_won%_std_l60_decay\",\"p_m_time_last\", \"p_tot_time_l6\", \"p_tot_time_l6_decay\", \"p_tot_pts_last\", \"p_tot_pts_l6\", \"p_tot_pts_l6_decay\", \"p_stamina_adj_fatigue\", \"p_stamina_adj_fatigue_decay\", \"high_t_ace_p_ace\", \"high_t_ace_p_aced\", \"p_opp_rank_diff\", \"p_opp_log_rank_diff\", \"p_opp_rank_pts_diff\", \"p_ent_diff\", \"p_opp_ht_diff\", \"p_opp_age_diff\", \"p_L_opp_R\", \"p_HCA_opp_N\", \"p_pts_won%_l60_decay_diff\", \"p_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted_diff\", \"p_pts_won%_l10_diff\", \"p_SOS_adj_pts_won%_l10_diff\", \"p_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_pts_won%_l60_decay_diff\", \"p_sv_pts_won%_l10_diff\", \"p_SOS_adj_sv_pts_won%_l10_diff\", \"p_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_pts_won%_l60_decay_diff\", \"p_ret_pts_won%_l10_diff\", \"p_SOS_adj_ret_pts_won%_l10_diff\", \"p_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_sv_opp_ret_pts_won%_l10_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l10_diff\", \"p_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_ret_opp_sv_pts_won%_l10_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l10_diff\", \"p_ace%_l60_decay_diff\", \"p_SOS_adj_ace%_l60_decay_diff\", \"p_ace%_l10_diff\", \"p_SOS_adj_ace%_l10_diff\", \"p_aced%_l60_decay_diff\", \"p_SOS_adj_aced%_l60_decay_diff\", \"p_aced%_l10_diff\", \"p_SOS_adj_aced%_l10_diff\", \"p_ace%_opp_aced%_l60_decay_diff\", \"p_SOS_adj_ace%_opp_aced%_l60_decay_diff\", \"p_ace%_opp_aced%_l10_diff\", \"p_SOS_adj_ace%_opp_aced%_l10_diff\", \"p_aced%_opp_ace%_l60_decay_diff\", \"p_SOS_adj_aced%_opp_ace%_l60_decay_diff\", \"p_aced%_opp_ace%_l10_diff\", \"p_SOS_adj_aced%_opp_ace%_l10_diff\", \"p_bp_save%_l60_diff\", \"p_SOS_adj_bp_save%_l60_diff\", \"p_bp_save%_l10_diff\", \"p_SOS_adj_bp_save%_l10_diff\", \"p_bp_convert%_l60_diff\", \"p_SOS_adj_bp_convert%_l60_diff\", \"p_bp_convert%_l10_diff\", \"p_SOS_adj_bp_convert%_l10_diff\", \"p_bp_convert%_opp_bp_save%_l60_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l60_diff\", \"p_bp_convert%_opp_bp_save%_l10_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l10_diff\", \"p_bp_save%_opp_bp_convert%_l60_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l60_diff\", \"p_bp_save%_opp_bp_convert%_l10_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l10_diff\", \"p_pts_won%_std_l60_decay_diff\", 'p_sv_pts_won%_std_l60_decay_diff','p_ret_pts_won%_std_l60_decay_diff', \"p_m_time_last_diff\", \"p_tot_time_l6_diff\", \"p_tot_time_l6_decay_diff\", \"p_tot_pts_last_diff\", \"p_tot_pts_l6_diff\", \"p_tot_pts_l6_decay_diff\", \"p_matches_surf_diff\", \"p_stam_adj_fatigue_diff\", \"p_stam_adj_fatigue_decay_diff\", \"p_H2H_diff\", \"p_H2H_pts_won%_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bcb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pared down to just the predictive features(both raw and player-opponent differential for match being predicted on), and the target feature itself ( player % pts won in the mtch being predicted on)\n",
    "# All features are derived from data available prior to any given match being predicted on. No data leakage!\n",
    "# This feature set DOES INCLUDE RAW implied odds (vig removed) from Dan Westin's historical wagering lines. See cell below for that iteration.\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_rank\", \"p_log_rank\", \"p_rank_pts\", \"p_ent\", \"p_hd\", \"p_ht\", \"p_age\", \"p_matches_surf\", \"p_H2H_w\", \"p_H2H_pts_won%\", \"p_pts_won%_l60_decay\", \"p_pts_won%_l60_decay_IO\", \"p_pts_won%_l10\", \"p_SOS_adj_pts_won%_l60_decay\", \"p_SOS_adj_pts_won%_l60_decay_IO\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted\", \"p_SOS_adj_pts_won%_l10\", \"p_sv_pts_won%_l60_decay\", \"p_sv_pts_won%_l10\", \"p_SOS_adj_sv_pts_won%_l60_decay\", \"p_SOS_adj_sv_pts_won%_l10\", \"p_ret_pts_won%_l60_decay\", \"p_ret_pts_won%_l10\", \"p_SOS_adj_ret_pts_won%_l60_decay\", \"p_SOS_adj_ret_pts_won%_l10\", \"p_ace%_l60_decay\", \"p_ace%_l10\", \"p_SOS_adj_ace%_l60_decay\", \"p_SOS_adj_ace%_l10\", \"p_aced%_l60_decay\", \"p_aced%_l10\", \"p_SOS_adj_aced%_l60_decay\", \"p_SOS_adj_aced%_l10\", \"p_bp_save%_l60\", \"p_bp_save%_l10\", \"p_SOS_adj_bp_save%_l60\", \"p_SOS_adj_bp_save%_l10\", \"p_bp_convert%_l60\", \"p_bp_convert%_l10\", \"p_SOS_adj_bp_convert%_l60\", \"p_SOS_adj_bp_convert%_l10\", \"p_pts_won%_std_l60_decay\",\"p_sv_pts_won%_std_l60_decay\", \"p_ret_pts_won%_std_l60_decay\",\"p_m_time_last\", \"p_tot_time_l6\", \"p_tot_time_l6_decay\", \"p_tot_pts_last\", \"p_tot_pts_l6\", \"p_tot_pts_l6_decay\", \"p_stamina_adj_fatigue\", \"p_stamina_adj_fatigue_decay\", \"high_t_ace_p_ace\", \"high_t_ace_p_aced\", \"p_opp_rank_diff\", \"p_opp_log_rank_diff\", \"p_opp_rank_pts_diff\", \"p_ent_diff\", \"p_opp_ht_diff\", \"p_opp_age_diff\", \"p_L_opp_R\", \"p_HCA_opp_N\", \"p_pts_won%_l60_decay_diff\", \"p_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted_diff\", \"p_pts_won%_l10_diff\", \"p_SOS_adj_pts_won%_l10_diff\", \"p_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_pts_won%_l60_decay_diff\", \"p_sv_pts_won%_l10_diff\", \"p_SOS_adj_sv_pts_won%_l10_diff\", \"p_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_pts_won%_l60_decay_diff\", \"p_ret_pts_won%_l10_diff\", \"p_SOS_adj_ret_pts_won%_l10_diff\", \"p_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_sv_opp_ret_pts_won%_l10_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l10_diff\", \"p_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_ret_opp_sv_pts_won%_l10_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l10_diff\", \"p_ace%_l60_decay_diff\", \"p_SOS_adj_ace%_l60_decay_diff\", \"p_ace%_l10_diff\", \"p_SOS_adj_ace%_l10_diff\", \"p_aced%_l60_decay_diff\", \"p_SOS_adj_aced%_l60_decay_diff\", \"p_aced%_l10_diff\", \"p_SOS_adj_aced%_l10_diff\", \"p_ace%_opp_aced%_l60_decay_diff\", \"p_SOS_adj_ace%_opp_aced%_l60_decay_diff\", \"p_ace%_opp_aced%_l10_diff\", \"p_SOS_adj_ace%_opp_aced%_l10_diff\", \"p_aced%_opp_ace%_l60_decay_diff\", \"p_SOS_adj_aced%_opp_ace%_l60_decay_diff\", \"p_aced%_opp_ace%_l10_diff\", \"p_SOS_adj_aced%_opp_ace%_l10_diff\", \"p_bp_save%_l60_diff\", \"p_SOS_adj_bp_save%_l60_diff\", \"p_bp_save%_l10_diff\", \"p_SOS_adj_bp_save%_l10_diff\", \"p_bp_convert%_l60_diff\", \"p_SOS_adj_bp_convert%_l60_diff\", \"p_bp_convert%_l10_diff\", \"p_SOS_adj_bp_convert%_l10_diff\", \"p_bp_convert%_opp_bp_save%_l60_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l60_diff\", \"p_bp_convert%_opp_bp_save%_l10_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l10_diff\", \"p_bp_save%_opp_bp_convert%_l60_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l60_diff\", \"p_bp_save%_opp_bp_convert%_l10_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l10_diff\", \"p_pts_won%_std_l60_decay_diff\", 'p_sv_pts_won%_std_l60_decay_diff','p_ret_pts_won%_std_l60_decay_diff', \"p_m_time_last_diff\", \"p_tot_time_l6_diff\", \"p_tot_time_l6_decay_diff\", \"p_tot_pts_last_diff\", \"p_tot_pts_l6_diff\", \"p_tot_pts_l6_decay_diff\", \"p_matches_surf_diff\", \"p_stam_adj_fatigue_diff\", \"p_stam_adj_fatigue_decay_diff\", \"p_H2H_diff\", \"p_H2H_pts_won%_diff\", \"p_IP_NV\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d56de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pared down to just the predictive features(both raw and player-opponent differential for match being predicted on), and the target feature itself ( player % pts won in the mtch being predicted on)\n",
    "# All features are derived from data available prior to any given match being predicted on. No data leakage!\n",
    "# This feature set DOES INCLUDE DIFFERENTIAL implied odds (vig removed) from Dan Westin's historical wagering lines. See cell below for that iteration.\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_rank\", \"p_log_rank\", \"p_rank_pts\", \"p_ent\", \"p_hd\", \"p_ht\", \"p_age\", \"p_matches_surf\", \"p_H2H_w\", \"p_H2H_pts_won%\", \"p_pts_won%_l60_decay\", \"p_pts_won%_l60_decay_IO\", \"p_pts_won%_l10\", \"p_SOS_adj_pts_won%_l60_decay\", \"p_SOS_adj_pts_won%_l60_decay_IO\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted\", \"p_SOS_adj_pts_won%_l10\", \"p_sv_pts_won%_l60_decay\", \"p_sv_pts_won%_l10\", \"p_SOS_adj_sv_pts_won%_l60_decay\", \"p_SOS_adj_sv_pts_won%_l10\", \"p_ret_pts_won%_l60_decay\", \"p_ret_pts_won%_l10\", \"p_SOS_adj_ret_pts_won%_l60_decay\", \"p_SOS_adj_ret_pts_won%_l10\", \"p_ace%_l60_decay\", \"p_ace%_l10\", \"p_SOS_adj_ace%_l60_decay\", \"p_SOS_adj_ace%_l10\", \"p_aced%_l60_decay\", \"p_aced%_l10\", \"p_SOS_adj_aced%_l60_decay\", \"p_SOS_adj_aced%_l10\", \"p_bp_save%_l60\", \"p_bp_save%_l10\", \"p_SOS_adj_bp_save%_l60\", \"p_SOS_adj_bp_save%_l10\", \"p_bp_convert%_l60\", \"p_bp_convert%_l10\", \"p_SOS_adj_bp_convert%_l60\", \"p_SOS_adj_bp_convert%_l10\", \"p_pts_won%_std_l60_decay\",\"p_sv_pts_won%_std_l60_decay\", \"p_ret_pts_won%_std_l60_decay\",\"p_m_time_last\", \"p_tot_time_l6\", \"p_tot_time_l6_decay\", \"p_tot_pts_last\", \"p_tot_pts_l6\", \"p_tot_pts_l6_decay\", \"p_stamina_adj_fatigue\", \"p_stamina_adj_fatigue_decay\", \"high_t_ace_p_ace\", \"high_t_ace_p_aced\", \"p_opp_rank_diff\", \"p_opp_log_rank_diff\", \"p_opp_rank_pts_diff\", \"p_ent_diff\", \"p_opp_ht_diff\", \"p_opp_age_diff\", \"p_L_opp_R\", \"p_HCA_opp_N\", \"p_pts_won%_l60_decay_diff\", \"p_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted_diff\", \"p_pts_won%_l10_diff\", \"p_SOS_adj_pts_won%_l10_diff\", \"p_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_pts_won%_l60_decay_diff\", \"p_sv_pts_won%_l10_diff\", \"p_SOS_adj_sv_pts_won%_l10_diff\", \"p_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_pts_won%_l60_decay_diff\", \"p_ret_pts_won%_l10_diff\", \"p_SOS_adj_ret_pts_won%_l10_diff\", \"p_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_sv_opp_ret_pts_won%_l10_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l10_diff\", \"p_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_ret_opp_sv_pts_won%_l10_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l10_diff\", \"p_ace%_l60_decay_diff\", \"p_SOS_adj_ace%_l60_decay_diff\", \"p_ace%_l10_diff\", \"p_SOS_adj_ace%_l10_diff\", \"p_aced%_l60_decay_diff\", \"p_SOS_adj_aced%_l60_decay_diff\", \"p_aced%_l10_diff\", \"p_SOS_adj_aced%_l10_diff\", \"p_ace%_opp_aced%_l60_decay_diff\", \"p_SOS_adj_ace%_opp_aced%_l60_decay_diff\", \"p_ace%_opp_aced%_l10_diff\", \"p_SOS_adj_ace%_opp_aced%_l10_diff\", \"p_aced%_opp_ace%_l60_decay_diff\", \"p_SOS_adj_aced%_opp_ace%_l60_decay_diff\", \"p_aced%_opp_ace%_l10_diff\", \"p_SOS_adj_aced%_opp_ace%_l10_diff\", \"p_bp_save%_l60_diff\", \"p_SOS_adj_bp_save%_l60_diff\", \"p_bp_save%_l10_diff\", \"p_SOS_adj_bp_save%_l10_diff\", \"p_bp_convert%_l60_diff\", \"p_SOS_adj_bp_convert%_l60_diff\", \"p_bp_convert%_l10_diff\", \"p_SOS_adj_bp_convert%_l10_diff\", \"p_bp_convert%_opp_bp_save%_l60_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l60_diff\", \"p_bp_convert%_opp_bp_save%_l10_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l10_diff\", \"p_bp_save%_opp_bp_convert%_l60_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l60_diff\", \"p_bp_save%_opp_bp_convert%_l10_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l10_diff\", \"p_pts_won%_std_l60_decay_diff\", 'p_sv_pts_won%_std_l60_decay_diff','p_ret_pts_won%_std_l60_decay_diff', \"p_m_time_last_diff\", \"p_tot_time_l6_diff\", \"p_tot_time_l6_decay_diff\", \"p_tot_pts_last_diff\", \"p_tot_pts_l6_diff\", \"p_tot_pts_l6_decay_diff\", \"p_matches_surf_diff\", \"p_stam_adj_fatigue_diff\", \"p_stam_adj_fatigue_decay_diff\", \"p_H2H_diff\", \"p_H2H_pts_won%_diff\", \"p_IP_NV_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pared down to just the predictive features(both raw and player-opponent differential for match being predicted on), and the target feature itself ( player % pts won in the mtch being predicted on)\n",
    "# All features are derived from data available prior to any given match being predicted on. No data leakage!\n",
    "# This feature set DOES INCLUDE RAW AND DIFFERENTIAL implied odds (vig removed) from Dan Westin's historical wagering lines. See cell below for that iteration.\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_rank\", \"p_log_rank\", \"p_rank_pts\", \"p_ent\", \"p_hd\", \"p_ht\", \"p_age\", \"p_matches_surf\", \"p_H2H_w\", \"p_H2H_pts_won%\", \"p_pts_won%_l60_decay\", \"p_pts_won%_l60_decay_IO\", \"p_pts_won%_l10\", \"p_SOS_adj_pts_won%_l60_decay\", \"p_SOS_adj_pts_won%_l60_decay_IO\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted\", \"p_SOS_adj_pts_won%_l10\", \"p_sv_pts_won%_l60_decay\", \"p_sv_pts_won%_l10\", \"p_SOS_adj_sv_pts_won%_l60_decay\", \"p_SOS_adj_sv_pts_won%_l10\", \"p_ret_pts_won%_l60_decay\", \"p_ret_pts_won%_l10\", \"p_SOS_adj_ret_pts_won%_l60_decay\", \"p_SOS_adj_ret_pts_won%_l10\", \"p_ace%_l60_decay\", \"p_ace%_l10\", \"p_SOS_adj_ace%_l60_decay\", \"p_SOS_adj_ace%_l10\", \"p_aced%_l60_decay\", \"p_aced%_l10\", \"p_SOS_adj_aced%_l60_decay\", \"p_SOS_adj_aced%_l10\", \"p_bp_save%_l60\", \"p_bp_save%_l10\", \"p_SOS_adj_bp_save%_l60\", \"p_SOS_adj_bp_save%_l10\", \"p_bp_convert%_l60\", \"p_bp_convert%_l10\", \"p_SOS_adj_bp_convert%_l60\", \"p_SOS_adj_bp_convert%_l10\", \"p_pts_won%_std_l60_decay\",\"p_sv_pts_won%_std_l60_decay\", \"p_ret_pts_won%_std_l60_decay\",\"p_m_time_last\", \"p_tot_time_l6\", \"p_tot_time_l6_decay\", \"p_tot_pts_last\", \"p_tot_pts_l6\", \"p_tot_pts_l6_decay\", \"p_stamina_adj_fatigue\", \"p_stamina_adj_fatigue_decay\", \"high_t_ace_p_ace\", \"high_t_ace_p_aced\", \"p_opp_rank_diff\", \"p_opp_log_rank_diff\", \"p_opp_rank_pts_diff\", \"p_ent_diff\", \"p_opp_ht_diff\", \"p_opp_age_diff\", \"p_L_opp_R\", \"p_HCA_opp_N\", \"p_pts_won%_l60_decay_diff\", \"p_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_diff\", \"p_SOS_adj_pts_won%_l60_decay_IO_weighted_diff\", \"p_pts_won%_l10_diff\", \"p_SOS_adj_pts_won%_l10_diff\", \"p_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_pts_won%_l60_decay_diff\", \"p_sv_pts_won%_l10_diff\", \"p_SOS_adj_sv_pts_won%_l10_diff\", \"p_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_pts_won%_l60_decay_diff\", \"p_ret_pts_won%_l10_diff\", \"p_SOS_adj_ret_pts_won%_l10_diff\", \"p_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l60_decay_diff\", \"p_sv_opp_ret_pts_won%_l10_diff\", \"p_SOS_adj_sv_opp_ret_pts_won%_l10_diff\", \"p_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l60_decay_diff\", \"p_ret_opp_sv_pts_won%_l10_diff\", \"p_SOS_adj_ret_opp_sv_pts_won%_l10_diff\", \"p_ace%_l60_decay_diff\", \"p_SOS_adj_ace%_l60_decay_diff\", \"p_ace%_l10_diff\", \"p_SOS_adj_ace%_l10_diff\", \"p_aced%_l60_decay_diff\", \"p_SOS_adj_aced%_l60_decay_diff\", \"p_aced%_l10_diff\", \"p_SOS_adj_aced%_l10_diff\", \"p_ace%_opp_aced%_l60_decay_diff\", \"p_SOS_adj_ace%_opp_aced%_l60_decay_diff\", \"p_ace%_opp_aced%_l10_diff\", \"p_SOS_adj_ace%_opp_aced%_l10_diff\", \"p_aced%_opp_ace%_l60_decay_diff\", \"p_SOS_adj_aced%_opp_ace%_l60_decay_diff\", \"p_aced%_opp_ace%_l10_diff\", \"p_SOS_adj_aced%_opp_ace%_l10_diff\", \"p_bp_save%_l60_diff\", \"p_SOS_adj_bp_save%_l60_diff\", \"p_bp_save%_l10_diff\", \"p_SOS_adj_bp_save%_l10_diff\", \"p_bp_convert%_l60_diff\", \"p_SOS_adj_bp_convert%_l60_diff\", \"p_bp_convert%_l10_diff\", \"p_SOS_adj_bp_convert%_l10_diff\", \"p_bp_convert%_opp_bp_save%_l60_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l60_diff\", \"p_bp_convert%_opp_bp_save%_l10_diff\", \"p_SOS_adj_bp_convert%_opp_bp_save%_l10_diff\", \"p_bp_save%_opp_bp_convert%_l60_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l60_diff\", \"p_bp_save%_opp_bp_convert%_l10_diff\", \"p_SOS_adj_bp_save%_opp_bp_convert%_l10_diff\", \"p_pts_won%_std_l60_decay_diff\", 'p_sv_pts_won%_std_l60_decay_diff','p_ret_pts_won%_std_l60_decay_diff', \"p_m_time_last_diff\", \"p_tot_time_l6_diff\", \"p_tot_time_l6_decay_diff\", \"p_tot_pts_last_diff\", \"p_tot_pts_l6_diff\", \"p_tot_pts_l6_decay_diff\", \"p_matches_surf_diff\", \"p_stam_adj_fatigue_diff\", \"p_stam_adj_fatigue_decay_diff\", \"p_H2H_diff\", \"p_H2H_pts_won%_diff\", \"p_IP_NV\", \"p_IP_NV_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY Raw Implied Odds As a Player Predictive Feature\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_IP_NV\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a98cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY Differential Implied Odds As a Player Predictive Feature\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_IP_NV_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa831a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY Differential AND RAW Implied Odds As Player Predictive Features\n",
    "df_model1 = df_filter2[[\"p_pts_won%\", \"t_indoor\", \"t_alt\", \"t_ace%_last\", \"t_lvl\", \"t_draw_size\", \"t_rd_num\", \"m_best_of\", \"p_IP_NV\", \"p_IP_NV_diff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c9936",
   "metadata": {},
   "source": [
    "### Data Split for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778504df",
   "metadata": {},
   "source": [
    "Identical split as for dummy/simple modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_model1) * .75, len(df_model1) * .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicates EXACT train-test split from dummy and simpler modeling\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model1.drop(columns='p_pts_won%'), \n",
    "                                                    df_model1[\"p_pts_won%\"], test_size=0.25, \n",
    "                                                    random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd84540",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51897b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a500f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300fd6a4",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bff8c",
   "metadata": {},
   "source": [
    "### Gradient Boosting Model: Hyperparameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22473aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pipeline \n",
    "GB_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    GradientBoostingRegressor(random_state= 47)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b53602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Grid Parameters. These were found to be optimal for prediction with the full feature set in the original Tennis Prediction Project\n",
    "\n",
    "n_est = [125]\n",
    "learning_rate = [.05]\n",
    "max_depth = [3]\n",
    "max_features = [6] #6 is best for full model\n",
    "\n",
    "\n",
    "grid_params = {\n",
    "        'gradientboostingregressor__n_estimators': n_est,\n",
    "        'standardscaler': [StandardScaler()],\n",
    "        'simpleimputer__strategy': ['median'],\n",
    "        'gradientboostingregressor__learning_rate': learning_rate,\n",
    "        'gradientboostingregressor__max_depth': max_depth,\n",
    "        'gradientboostingregressor__max_features': max_features\n",
    "}\n",
    "grid_params\n",
    "\n",
    "#Run 1\n",
    "#GridSearchCV(cv=5,\n",
    "             #estimator=Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
    "                                       #('standardscaler', StandardScaler()),\n",
    "                                       #('gradientboostingregressor',\n",
    "                                       # GradientBoostingRegressor(random_state=47))]),\n",
    "             #param_grid={'gradientboostingregressor__learning_rate': [0.05,\n",
    "                                                                      #0.06],\n",
    "                         #'gradientboostingregressor__max_depth': [2, 3],\n",
    "                         #'gradientboostingregressor__max_features': [4, 6, 8],\n",
    "                         #'gradientboostingregressor__n_estimators': [100, 110,\n",
    "                                                                     #120],\n",
    "                         #'simpleimputer__strategy': ['median'],\n",
    "                         #'standardscaler': [StandardScaler()]})\n",
    "                            \n",
    "#Run 2\n",
    "#GridSearchCV(cv=5,\n",
    "             #estimator=Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
    "                                       #('standardscaler', StandardScaler()),\n",
    "                                       #('gradientboostingregressor',\n",
    "                                       #GradientBoostingRegressor(random_state=47))]),\n",
    "            #param_grid={'gradientboostingregressor__learning_rate': [0.04,\n",
    "                                                                      #0.05],\n",
    "                         #'gradientboostingregressor__max_depth': [3, 4],\n",
    "                         #'gradientboostingregressor__max_features': [5, 6, 7],\n",
    "                         #'gradientboostingregressor__n_estimators': [115, 120,\n",
    "                                                                     #125],\n",
    "                         #'simpleimputer__strategy': ['median'],\n",
    "                         #'standardscaler': [StandardScaler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call `GridSearchCV` with the gradient boosting pipeline, passing in the above `grid_params`\n",
    "#dict for parameters to evaluate, 5-fold cross-validation\n",
    "gb_grid_cv = GridSearchCV(GB_pipe, param_grid=grid_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conduct the grid search. \n",
    "gb_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d15a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best params (`best_params_` attribute) from the grid search\n",
    "gb_grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f35b3",
   "metadata": {},
   "source": [
    "### Best Gradient Boosting Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820c285",
   "metadata": {},
   "source": [
    "#### R-squared (COD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc56798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation defaults to R^2 metric for scoring regression\n",
    "gb_best_cv_results = cross_validate(gb_grid_cv.best_estimator_, X_train, y_train, cv=5)\n",
    "gb_best_scores = gb_best_cv_results['test_score']\n",
    "gb_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "np.mean(gb_best_scores), np.std(gb_best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91478b1",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_neg_mae = cross_validate(gb_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd81789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "gb_mae_mean = np.mean(-1 * gb_neg_mae['test_score'])\n",
    "gb_mae_std = np.std(-1 * gb_neg_mae['test_score'])\n",
    "gb_mae_mean, gb_mae_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d131fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "mean_absolute_error(y_test, gb_grid_cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e05fa0",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680649e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_neg_mse = cross_validate(gb_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "gb_mse_mean = np.mean(-1 * gb_neg_mse['test_score'])\n",
    "gb_mse_std = np.std(-1 * gb_neg_mse['test_score'])\n",
    "gb_mse_mean, gb_mse_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "mean_squared_error(y_test, gb_grid_cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e2fa6",
   "metadata": {},
   "source": [
    "#### Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_neg_rmse = cross_validate(gb_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_root_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "gb_rmse_mean = np.mean(-1 * gb_neg_rmse['test_score'])\n",
    "gb_rmse_std = np.std(-1 * gb_neg_rmse['test_score'])\n",
    "gb_rmse_mean, gb_rmse_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "np.sqrt(mean_squared_error(y_test, gb_grid_cv.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42bbe6",
   "metadata": {},
   "source": [
    "### Best Gradient Boosting Model Feature Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a barplot of the gradient boosting model's feature importances,\n",
    "#assigning the `feature_importances_` attribute of \n",
    "#`gb_grid_cv.best_estimator_.named_steps.gradientboostingregressor` to the name `imps` to then\n",
    "#create a pandas Series object of the feature importances, with the index given by the\n",
    "#training data column names, sorting the values in descending order\n",
    "plt.subplots(figsize=(20, 10))\n",
    "imps = gb_grid_cv.best_estimator_.named_steps.gradientboostingregressor.feature_importances_\n",
    "rf_feat_imps = pd.Series(imps, index=X_train.columns).sort_values(ascending=False)\n",
    "rf_feat_imps.plot(kind='bar')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('importance')\n",
    "plt.title('Best gradient boosting regressor feature importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2ad1e",
   "metadata": {},
   "source": [
    "### Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c66adc",
   "metadata": {},
   "source": [
    "Because Gradient Boosting Regressor turned out to be the best of the four models tested (in terms of cross-validation RMSE) for hard courts, a Data Quality Assessment was run on this model to ensure that results are not hindered by sample size. Much more analysis of data size and year range of inclusion is conducted in the final reporting (see Reporting folder of original Tennis Prediction Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, .9, 1.0]\n",
    "train_size, train_scores, test_scores = learning_curve(GB_pipe, X_train, y_train, train_sizes=fractions)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ef1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 5))\n",
    "plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('CV scores')\n",
    "plt.title('Cross-validation score as training set size increases');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129971e3",
   "metadata": {},
   "source": [
    "## Save Best Model Object From Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gb_grid_cv.best_estimator_\n",
    "best_model.version = '1.0'\n",
    "best_model.pandas_version = pd.__version__\n",
    "best_model.numpy_version = np.__version__\n",
    "best_model.sklearn_version = sklearn_version\n",
    "best_model.X_columns = [col for col in X_train.columns]\n",
    "best_model.build_datetime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "modelpath = '../models'\n",
    "save_file(best_model, 'tennis_HC_model.pkl', modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a907384",
   "metadata": {},
   "source": [
    "## New Conclusions \n",
    "\n",
    "#### Best Full Model Performance WITHOUT Closing Match Odds-Derived Implied Probabilities \n",
    "* The best regression model (also tested were Linear, Random Forest, Hist Gradient Boosting) as determined in the original Tennis Prediction Project (Gradient Boosting), making use of the full set of raw (matchup-independent) and differential (player relative to the other player in the match being predicted on) predictive features: \n",
    "    * The best full model resulted in RMSE(STD): 5.40% (.04%) for training set cross validation and 5.38% for the test set.\n",
    "\n",
    "#### Best Full Model Performance WITH Closing Match Odds-Derived Implied Probabilities (+ Raw Only)\n",
    "* RMSE(STD): 5.39% (.03) for training set cross validation and 5.36% for the test set. \n",
    "    * So adding the closing lines-derived implied odds as a feature to the best model slightly improved model performance\n",
    "\n",
    "#### Best Full Model Performance WITH Closing Match Odds-Derived Implied Probabilities (+ Differential Only)\n",
    "* RMSE(STD): 5.39% (.03) for training set cross validation and 5.36% for the test set. \n",
    "    * So adding the closing lines-derived implied odds as a feature to the best model slightly improved model performance\n",
    "\n",
    "#### Best Full Model Performance WITH Closing Match Odds-Derived Implied Probabilities (+ Raw AND Differential)\n",
    "* RMSE(STD): 5.35% (.02) for training set cross validation and 5.33% for the test set. \n",
    "    * So adding the closing lines-derived implied odds as a feature to the best model slightly improved model performance\n",
    "\n",
    "\n",
    "#### Model Performance With ONLY RAW Closing Match Odds-Derived Implied Probabilities as a Predictive Feature\n",
    "* RMSE(STD): 5.31% (.02%) for training set cross validation and 5.29% for the test set.\n",
    "    * So, slightly better than the full model, but not incredibly so!\n",
    "\n",
    "#### Model Performance With ONLY DIFFERENTIAL Closing Match Odds-Derived Implied Probabilities as a Predictive Feature\n",
    "* RMSE(STD): 5.31% (.02%) for training set cross validation and 5.29% for the test set.\n",
    "    * So, slightly better than the full model, but not incredibly so!\n",
    "    \n",
    "#### Model Performance W/ ONLY RAW AND DIFFERENTIAL Closing Match Odds-Derived Implied Probabilities as Predictive Features\n",
    "* RMSE(STD): 5.31% (.01%) for training set cross validation and 5.29% for the test set.\n",
    "    * So, slightly better than the full model, but not incredibly so!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
